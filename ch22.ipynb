{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137d0758-50a3-41fb-ac97-efeb2e898174",
   "metadata": {},
   "source": [
    "# 22장. 비정형 데이터를 위한 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b50fa-53a2-4286-b443-0302600cc56f",
   "metadata": {},
   "source": [
    "<table align=\"left\"><tr><td>\n",
    "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-python-cookbook-2nd/blob/main/ch22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"코랩에서 실행하기\"/></a>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c60fea-e573-435d-b430-c15cc513e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "print('numpy', np.__version__)\n",
    "print('sklearn', sklearn.__version__)\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)\n",
    "print('transformers', transformers.__version__)\n",
    "print('datasets', datasets.__version__)\n",
    "print('evaluate', evaluate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1343f-1628-4655-8169-9786e8f4cbbb",
   "metadata": {},
   "source": [
    "## 22.1 이미지 분류 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0057e7-de58-4195-8bf9-003e43a5f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 합성곱 신경망을 정의합니다.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(self.dropout1(x), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(self.dropout2(x)))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# 실행 장치를 설정합니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터 전처리 단계를 정의합니다.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋을 로드합니다.\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "    transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# 데이터 로더를 정의합니다.\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "# 모델과 옵티마이저를 초기화합니다.\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model = torch.compile(model)\n",
    "\n",
    "# 훈련 루프를 정의합니다.\n",
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = nn.functional.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 테스트 루프를 정의합니다.\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        # 가장 큰 로그 확률의 인덱스를 추출합니다.\n",
    "        test_loss += nn.functional.nll_loss(\n",
    "            output, target, reduction='sum'\n",
    "        ).item()  # 배치 손실을 더합니다.\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06991fcc-6644-4a69-a98b-18a4c01475a4",
   "metadata": {},
   "source": [
    "## 22.2 텍스트 분류 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cefc2-ac15-4e31-9465-b39d60bc9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 20 뉴스그룹 데이터셋을 로드합니다.\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', shuffle=True,\n",
    "    random_state=42, categories=cats)\n",
    "\n",
    "# 훈련 세트와 테스트 세트를 만듭니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data,\n",
    "    newsgroups_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# bag-of-words 방식을 사용해 텍스트 데이터를 벡터화합니다.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# 데이터를 파이토치 텐서로 변환합니다.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 모델을 정의합니다.\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# 모델을 초기화하고, 손실 함수와 옵티마이저를 정의합니다.\n",
    "model = TextClassifier(num_classes=len(cats))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model = torch.compile(model)\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "num_batches = len(X_train) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        # 현재 배치를 위한 입력과 타깃 데이터를 준비합니다.\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        inputs = X_train[start_idx:end_idx]\n",
    "        targets = y_train[start_idx:end_idx]\n",
    "\n",
    "        # 옵티마이저의 그레이디언트를 0으로 초기화합니다.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델의 정방향 계산을 수행하고 손실을 계산합니다.\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Backward pass through the model and update the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 해당 에포크의 총 손실을 업데이트합니다.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 해당 에포크에 대한 테스트 세트의 정확도를 계산합니다.\n",
    "    test_outputs = model(X_test)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "    # 에포크 횟수, 평균 손실, 테스트 세트 정확도를 출력합니다.\n",
    "    print(f\"에포크: {epoch+1}, 손실: {total_loss/num_batches}, 테스트 세트 정확도:\"\n",
    "        \"{test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4103d6-f4ba-4d74-99f2-103f54927001",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676757f5-3e97-405e-a122-08ff0325871e",
   "metadata": {},
   "source": [
    "## 22.3 이미지 분류를 위해 사전 훈련된 모델 미세 튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4fea0-73c2-480f-a4cf-8b9eb92fb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "import torch\n",
    "from torchvision.transforms import(\n",
    "    RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "    )\n",
    "from transformers import Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from datasets import load_dataset, load_metric, Image\n",
    "\n",
    "# 이미지를 RGB로 변환하기 위한 헬퍼 함수를 정의합니다.\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in\n",
    "        examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# 측정 지표를 계산하기 위한 헬퍼 함수를 정의합니다.\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1),\n",
    "        references=p.label_ids)\n",
    "\n",
    "# 패션 mnist 데이터셋을 로드합니다.\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "# VIT 모델에서 전처리기를 로드합니다.\n",
    "image_processor = ViTFeatureExtractor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "# 데이터셋에서 레이블을 추출합니다.\n",
    "labels = dataset['train'].features['label'].names\n",
    "\n",
    "# 사전 훈련된 모델을 로드합니다.\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "\n",
    "# 콜레이터, 정규화를 정의하고 변환합니다.\n",
    "collate_fn = DefaultDataCollator()\n",
    "normalize = Normalize(mean=image_processor.image_mean,\n",
    "    std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "# 변환할 데이터셋을 로드합니다.\n",
    "dataset = dataset.with_transform(transforms)\n",
    "\n",
    "# 정확도를 측정 지표로 사용합니다.\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# 훈련 매개변수를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fashion_mnist_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# trainer를 초기화합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "# 모델을 기록하고 지표를 기록, 저장합니다.\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d19306-c563-4f3c-9ca6-a489dcf03a4b",
   "metadata": {},
   "source": [
    "## 22.4 텍스트 분류를 위해 사전 훈련된 모델 미세 튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7abab-c2b3-41cc-a6b0-c88c4677cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    )\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# imdb 데이터셋을 로드합니다.\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# 토크나이저와 콜레이터를 만듭니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# imdb 데이터셋을 토큰화합니다.\n",
    "tokenized_imdb = imdb.map(\n",
    "    lambda example: tokenizer(\n",
    "        example[\"text\"], padding=\"max_length\", truncation=True\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# 정확도 지표를 사용합니다.\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 지표를 계산하는 헬퍼 함수를 정의합니다.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 인덱스와 레이블을 서로 매핑하는 딕셔너리를 만듭니다.\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "# 사전 훈련된 모델을 로드합니다.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label,\n",
    "        label2id=label2id\n",
    ")\n",
    "\n",
    "# 훈련 매개변수를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# trainer를 초기화합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
